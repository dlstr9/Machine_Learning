{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3aAmACeDhAk_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = MNIST(root='./data', train=False, transform=transform, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvD-5cI7hHI-",
        "outputId": "c5584442-817e-472e-f81b-01f94f848926"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 82931157.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 27227622.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 19972539.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3999691.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "5J652KvjhViX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a wider and deeper neural network model\n",
        "class WiderDeeperNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
        "        super(WiderDeeperNet, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        prev_size = input_size\n",
        "\n",
        "        # Create a wider and deeper architecture\n",
        "        for hidden_size in hidden_sizes:\n",
        "            self.layers.append(nn.Linear(prev_size, hidden_size))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            prev_size = hidden_size\n",
        "        self.fc_out = nn.Linear(prev_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        out = self.fc_out(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "8w0ZzY5ohdD1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train and evaluate the model\n",
        "def train_and_evaluate(model, train_loader, test_loader, num_epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        predicted_labels = []\n",
        "        true_labels = []\n",
        "\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            predicted_labels.extend(predicted.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "        f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "        precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "        recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "    return accuracy, f1, precision, recall"
      ],
      "metadata": {
        "id": "20BZP3tohzeV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define three different model configurations\n",
        "hidden_sizes_config1 = [128, 64]  # Wider and deeper\n",
        "hidden_sizes_config2 = [256, 128, 64]  # Wider and deeper\n",
        "hidden_sizes_config3 = [64, 32]  # Shallower and narrower\n",
        "\n",
        "# Train and evaluate each configuration\n",
        "model1 = WiderDeeperNet(input_size=28*28, hidden_sizes=hidden_sizes_config1, num_classes=10)\n",
        "accuracy1, f1_1, precision1, recall1 = train_and_evaluate(model1, train_loader, test_loader)\n",
        "\n",
        "model2 = WiderDeeperNet(input_size=28*28, hidden_sizes=hidden_sizes_config2, num_classes=10)\n",
        "accuracy2, f1_2, precision2, recall2 = train_and_evaluate(model2, train_loader, test_loader)\n",
        "\n",
        "model3 = WiderDeeperNet(input_size=28*28, hidden_sizes=hidden_sizes_config3, num_classes=10)\n",
        "accuracy3, f1_3, precision3, recall3 = train_and_evaluate(model3, train_loader, test_loader)\n",
        "\n",
        "# Print and explain the performance results\n",
        "print(\"Performance Results:\")\n",
        "print(f\"Configuration 1: Wider and Deeper Model - Accuracy: {accuracy1:.4f}, F1 Score: {f1_1:.4f}, Precision: {precision1:.4f}, Recall: {recall1:.4f}\")\n",
        "print(f\"Configuration 2: Wider and Deeper Model - Accuracy: {accuracy2:.4f}, F1 Score: {f1_2:.4f}, Precision: {precision2:.4f}, Recall: {recall2:.4f}\")\n",
        "print(f\"Configuration 3: Shallower and Narrower Model - Accuracy: {accuracy3:.4f}, F1 Score: {f1_3:.4f}, Precision: {precision3:.4f}, Recall: {recall3:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLXQo9kPh556",
        "outputId": "87a79125-f3fd-4fb9-e17a-54782abb8572"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Results:\n",
            "Configuration 1: Wider and Deeper Model - Accuracy: 0.9662, F1 Score: 0.9663, Precision: 0.9673, Recall: 0.9662\n",
            "Configuration 2: Wider and Deeper Model - Accuracy: 0.9781, F1 Score: 0.9781, Precision: 0.9782, Recall: 0.9781\n",
            "Configuration 3: Shallower and Narrower Model - Accuracy: 0.9693, F1 Score: 0.9693, Precision: 0.9695, Recall: 0.9693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Result:"
      ],
      "metadata": {
        "id": "Bx8e5dWMkR2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari hasil performance results menggunakan dataset ini dapat disimpulkan bahwa configurasi dengan skala yang lebih besar dan dalam akan mendapatkan nilai akurasi yang lebih tinggi dibandingkan dengan skala yang lebih kecil dan *not deeper*. Bisa di lihat jika code di eksekusi Configuration 2 lah yang mendapatkan nilai Accuracy, F1 Score, Precision, Recall lebih besar dibandingkan yang lainnya."
      ],
      "metadata": {
        "id": "ovE0o30skU7T"
      }
    }
  ]
}